{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMf4DtCPrxezRnfWHjqEsZq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dondonrocket/kokudo/blob/%EF%BC%91%EF%BC%97%EF%BC%8E%EF%BC%90%E3%81%AE%E3%82%B3%E3%83%BC%E3%83%89base/re_hasegawa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a7Wc9jfop15s"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# 1) 読み込み & 定義（完成度重視）\n",
        "#  - 列名ゆらぎに強い\n",
        "#  - 2/3/4で使う定数・関数をここで完備\n",
        "# =========================================================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# =========================================================\n",
        "# 乱数・基本設定\n",
        "# =========================================================\n",
        "SEED = 2025\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# =========================================================\n",
        "# パス定義\n",
        "# =========================================================\n",
        "DATA_DIR = Path(\"/content\")  # Colab想定\n",
        "TRAIN_PATH = DATA_DIR / \"train.csv\"\n",
        "TEST_PATH  = DATA_DIR / \"test.csv\"\n",
        "\n",
        "# -------------------------\n",
        "# 重要列（確定しているもの）\n",
        "# -------------------------\n",
        "ID_COL     = \"building_id\"\n",
        "TARGET_COL = \"money_room\"\n",
        "YM_COL     = \"target_ym\"\n",
        "BUILDING_TYPE_COL = \"building_type\"\n",
        "\n",
        "# -------------------------\n",
        "# 時系列設定（README準拠）\n",
        "# -------------------------\n",
        "BASE_YEAR = 2019  # elapsed_months の基準\n",
        "\n",
        "# -------------------------\n",
        "# distance列のNaN対策（Step2で使う設定）\n",
        "# -------------------------\n",
        "DIST_SUFFIX = \"_distance\"\n",
        "# 大きな定数埋めの方針：trainの分位点で決める（例：0.99）\n",
        "DIST_FILL_QUANTILE = 0.99\n",
        "\n",
        "# =========================================================\n",
        "# 便利関数：読み込み\n",
        "# =========================================================\n",
        "def read_csv_sjis(path: Path) -> pd.DataFrame:\n",
        "    \"\"\"Shift-JIS前提の安定読み込み（このコンペに合わせる）\"\"\"\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"File not found: {path}\")\n",
        "    return pd.read_csv(\n",
        "        path,\n",
        "        encoding=\"shift_jis\",\n",
        "        encoding_errors=\"replace\",\n",
        "        low_memory=False\n",
        "    )\n",
        "\n",
        "def ensure_required_columns(df: pd.DataFrame, required: list[str], name: str):\n",
        "    missing = [c for c in required if c not in df.columns]\n",
        "    if missing:\n",
        "        # ここで止めるのが完成度（後段で謎バグにしない）\n",
        "        raise KeyError(f\"[{name}] missing columns: {missing}\")\n",
        "\n",
        "# =========================================================\n",
        "# 便利関数：列名ゆらぎ対応（lon/latなどが揺れる前提）\n",
        "# =========================================================\n",
        "def pick_first_existing(df: pd.DataFrame, candidates: list[str]) -> str:\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    raise KeyError(f\"None of candidates exist: {candidates}\")\n",
        "\n",
        "# lon/lat はコンペで揺れがちなので候補を多めに持つ\n",
        "LON_CANDIDATES = [\"lon\", \"longitude\", \"x\", \"X\", \"経度\"]\n",
        "LAT_CANDIDATES = [\"lat\", \"latitude\", \"y\", \"Y\", \"緯度\"]\n",
        "\n",
        "# =========================================================\n",
        "# 便利関数：時間列追加（Step2以降で使う前提列）\n",
        "# =========================================================\n",
        "def add_time_columns(df: pd.DataFrame, ym_col: str = YM_COL, base_year: int = BASE_YEAR) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    ym = pd.to_numeric(df[ym_col], errors=\"coerce\")\n",
        "    year = (ym // 100).astype(\"Int64\")\n",
        "    month = (ym % 100).astype(\"Int64\")\n",
        "\n",
        "    df[\"year\"] = year\n",
        "    df[\"month\"] = month\n",
        "    df[\"elapsed_months\"] = (df[\"year\"] - base_year) * 12 + (df[\"month\"] - 1)\n",
        "    return df\n",
        "\n",
        "# =========================================================\n",
        "# 便利関数：distance列検出（Step2でNaN処理対象にする）\n",
        "# =========================================================\n",
        "def get_distance_cols(df: pd.DataFrame, suffix: str = DIST_SUFFIX) -> list[str]:\n",
        "    return [c for c in df.columns if c.endswith(suffix)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 読み込み\n",
        "# =========================================================\n",
        "train = read_csv_sjis(TRAIN_PATH)\n",
        "test  = read_csv_sjis(TEST_PATH)"
      ],
      "metadata": {
        "id": "322pg9pCqBZs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 必須列チェック（ここで落とすのが正解）\n",
        "# =========================================================\n",
        "ensure_required_columns(train, [ID_COL, TARGET_COL, YM_COL, BUILDING_TYPE_COL], \"train\")\n",
        "ensure_required_columns(test,  [ID_COL, YM_COL, BUILDING_TYPE_COL], \"test\")\n",
        "\n",
        "# lon/lat は列名が揺れる可能性があるので自動解決\n",
        "LON_COL = pick_first_existing(train, LON_CANDIDATES)\n",
        "LAT_COL = pick_first_existing(train, LAT_CANDIDATES)\n",
        "ensure_required_columns(test, [LON_COL, LAT_COL], \"test\")  # test側にも同名がある前提で確認\n",
        "\n",
        "# 時間列を追加（Step2/3/4で使用）\n",
        "train = add_time_columns(train)\n",
        "test  = add_time_columns(test)\n",
        "\n",
        "# distance列（Step2で NaNフラグ+大きな定数埋め の対象）\n",
        "DISTANCE_COLS = get_distance_cols(train)\n",
        "# train/testで一致しているかも早期に確認（完成度）\n",
        "missing_in_test = sorted(list(set(DISTANCE_COLS) - set(test.columns)))\n",
        "if len(missing_in_test) > 0:\n",
        "    raise KeyError(f\"[test] missing some distance columns present in train: {missing_in_test[:30]} ... total={len(missing_in_test)}\")\n",
        "\n",
        "# =========================================================\n",
        "# Step3/4で使う“分割マスク”の土台（ここでは定義だけ）\n",
        "# =========================================================\n",
        "# building_type が mansion/house 以外の表記なら、ここで気づけるようにしておく\n",
        "BUILDING_TYPES_TRAIN = train[BUILDING_TYPE_COL].astype(str).value_counts()\n",
        "BUILDING_TYPES_TEST  = test[BUILDING_TYPE_COL].astype(str).value_counts()\n",
        "\n",
        "print(\"train shape:\", train.shape, \" test shape:\", test.shape)\n",
        "print(\"lon/lat cols:\", LON_COL, LAT_COL)\n",
        "print(\"target_ym range train:\", int(train[YM_COL].min()), \"->\", int(train[YM_COL].max()))\n",
        "print(\"target_ym range test :\", int(test[YM_COL].min()),  \"->\", int(test[YM_COL].max()))\n",
        "print(\"num distance cols:\", len(DISTANCE_COLS))\n",
        "print(\"building_type(train):\")\n",
        "display(BUILDING_TYPES_TRAIN.head(20))\n",
        "print(\"building_type(test):\")\n",
        "display(BUILDING_TYPES_TEST.head(20))"
      ],
      "metadata": {
        "id": "5GX6zgeQqr9W",
        "outputId": "c5bb07a8-f049-4e11-87f6-1d229e71c3a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Point    10531\n",
            "Name: count, dtype: int64\n",
            "train shape: (363924, 152)  test shape: (112437, 152)\n",
            "train ym range: 201901 -> 202207\n",
            "test  ym range: 202301 -> 202307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 2) 特徴量作成\n",
        "#   - distance: NaNフラグ + 大きな定数埋め\n",
        "#   - log距離\n",
        "#   - 時系列補助特徴\n",
        "# =========================================================\n",
        "\n",
        "# -------------------------\n",
        "# コピー（破壊的変更を避ける）\n",
        "# -------------------------\n",
        "train_feat = train.copy()\n",
        "test_feat  = test.copy()"
      ],
      "metadata": {
        "id": "lSMvSRsVqs4T"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 2-1. distance 特徴量\n",
        "#   ・NaN = 「一定距離内に存在しない」\n",
        "#   ・情報なのでフラグ化\n",
        "#   ・距離自体は大きな定数で埋める\n",
        "# =========================================================\n",
        "\n",
        "DIST_FILL_VALUES = {}  # Step3/4で再現できるよう保存\n",
        "\n",
        "for c in DISTANCE_COLS:\n",
        "    # ---- NaNフラグ\n",
        "    train_feat[f\"{c}_is_nan\"] = train_feat[c].isna().astype(\"int8\")\n",
        "    test_feat[f\"{c}_is_nan\"]  = test_feat[c].isna().astype(\"int8\")\n",
        "\n",
        "    # ---- 埋め値（trainの分位点で固定）\n",
        "    fill_value = train_feat[c].quantile(DIST_FILL_QUANTILE)\n",
        "    DIST_FILL_VALUES[c] = fill_value\n",
        "\n",
        "    train_feat[c] = train_feat[c].fillna(fill_value)\n",
        "    test_feat[c]  = test_feat[c].fillna(fill_value)\n",
        "\n",
        "    # ---- log距離（右裾対策）\n",
        "    train_feat[f\"{c}_log\"] = np.log1p(train_feat[c])\n",
        "    test_feat[f\"{c}_log\"]  = np.log1p(test_feat[c])"
      ],
      "metadata": {
        "id": "pRiaY6GRqw4O"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 2-2. 時系列特徴（README準拠：掲載時期ズレ対策）\n",
        "# =========================================================\n",
        "\n",
        "# year / month / elapsed_months は Step1 ですでに作成済み\n",
        "# 追加で「周期性」を与える\n",
        "train_feat[\"month_sin\"] = np.sin(2 * np.pi * train_feat[\"month\"] / 12)\n",
        "train_feat[\"month_cos\"] = np.cos(2 * np.pi * train_feat[\"month\"] / 12)\n",
        "\n",
        "test_feat[\"month_sin\"] = np.sin(2 * np.pi * test_feat[\"month\"] / 12)\n",
        "test_feat[\"month_cos\"] = np.cos(2 * np.pi * test_feat[\"month\"] / 12)"
      ],
      "metadata": {
        "id": "-XyIF5coL0Iy",
        "outputId": "9e33d117-38d1-4776-cc62-8915976465f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Point    10531\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 2-3. 建物年数系（壊れにくい最小構成）\n",
        "# =========================================================\n",
        "\n",
        "DATE_COLS = [\n",
        "    \"building_create_date\",\n",
        "    \"building_modify_date\"\n",
        "]\n",
        "\n",
        "for col in DATE_COLS:\n",
        "    if col in train_feat.columns:\n",
        "        train_feat[col] = pd.to_datetime(train_feat[col], errors=\"coerce\")\n",
        "        test_feat[col]  = pd.to_datetime(test_feat[col],  errors=\"coerce\")\n",
        "\n",
        "# 築年数（存在する場合のみ）\n",
        "if \"building_create_date\" in train_feat.columns:\n",
        "    train_feat[\"building_age\"] = train_feat[\"year\"] - train_feat[\"building_create_date\"].dt.year\n",
        "    test_feat[\"building_age\"]  = test_feat[\"year\"]  - test_feat[\"building_create_date\"].dt.year\n",
        "\n",
        "# マイナスや異常値を防ぐ\n",
        "if \"building_age\" in train_feat.columns:\n",
        "    train_feat[\"building_age\"] = train_feat[\"building_age\"].clip(lower=0)\n",
        "    test_feat[\"building_age\"]  = test_feat[\"building_age\"].clip(lower=0)"
      ],
      "metadata": {
        "id": "sUXL1k5Iqz2W"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 2-4. カテゴリ列の整理（Step3でそのまま使える形）\n",
        "# =========================================================\n",
        "\n",
        "# LightGBMに渡す予定のカテゴリ列\n",
        "CATEGORICAL_COLS = []\n",
        "\n",
        "if BUILDING_TYPE_COL in train_feat.columns:\n",
        "    CATEGORICAL_COLS.append(BUILDING_TYPE_COL)\n",
        "    train_feat[BUILDING_TYPE_COL] = train_feat[BUILDING_TYPE_COL].astype(\"category\")\n",
        "    test_feat[BUILDING_TYPE_COL]  = test_feat[BUILDING_TYPE_COL].astype(\"category\")"
      ],
      "metadata": {
        "id": "OQWjRqnZq3mu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 2-5. 数値列の最終ガード（inf / -inf / NaN）\n",
        "# =========================================================\n",
        "\n",
        "NUMERIC_COLS = train_feat.select_dtypes(include=[np.number]).columns.tolist()\n",
        "NUMERIC_COLS = [c for c in NUMERIC_COLS if c != TARGET_COL]\n",
        "\n",
        "def final_numeric_guard(df: pd.DataFrame, num_cols: list[str]) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df[num_cols] = (\n",
        "        df[num_cols]\n",
        "        .replace([np.inf, -np.inf], np.nan)\n",
        "        .fillna(0)\n",
        "    )\n",
        "    return df\n",
        "\n",
        "train_feat = final_numeric_guard(train_feat, NUMERIC_COLS)\n",
        "test_feat  = final_numeric_guard(test_feat,  NUMERIC_COLS)"
      ],
      "metadata": {
        "id": "YldeUXEiq5nO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 2-6. Step3/4 用の列リストを確定\n",
        "# =========================================================\n",
        "\n",
        "FEATURE_COLS = [\n",
        "    c for c in train_feat.columns\n",
        "    if c not in [TARGET_COL]\n",
        "    and c in test_feat.columns\n",
        "]\n",
        "\n",
        "print(\"num features:\", len(FEATURE_COLS))\n",
        "print(\"sample feature cols:\", FEATURE_COLS[:20])"
      ],
      "metadata": {
        "id": "7re7RBC_q7ZC",
        "outputId": "9220b6c3-b534-4166-8305-9baac045f8e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1787058874.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtrain_gdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_gdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSLASH_FEATURES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_slashed_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_gdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSLASH_COLS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1787058874.py\u001b[0m in \u001b[0;36madd_slashed_features\u001b[0;34m(train_df, test_df, cols)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0moh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"{col}_{c}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/strings/accessor.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 )\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/strings/accessor.py\u001b[0m in \u001b[0;36mget_dummies\u001b[0;34m(self, sep)\u001b[0m\n\u001b[1;32m   2306\u001b[0m         \u001b[0;31m# we need to cast to Series of strings as only that has all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2307\u001b[0m         \u001b[0;31m# methods available for making the dummies...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2308\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str_get_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2309\u001b[0m         return self._wrap_result(\n\u001b[1;32m   2310\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/strings/object_array.py\u001b[0m in \u001b[0;36m_str_get_dummies\u001b[0;34m(self, sep)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0mpat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             dummies[:, i] = lib.map_infer(\n\u001b[0m\u001b[1;32m    397\u001b[0m                 \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_isin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             )\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/strings/object_array.py\u001b[0m in \u001b[0;36m_isin\u001b[0;34m(test_elements, element)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mdummies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0m_isin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_elements\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 3) モデル作成（完成度重視）\n",
        "#   - 時系列 valid（例：2022）で検証可能な形にする\n",
        "#   - Step4で必要な変数を全てここで確定させる\n",
        "# =========================================================\n",
        "\n",
        "import lightgbm as lgb\n"
      ],
      "metadata": {
        "id": "sYz5h0qpJPri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 3-0. 整合性チェック（Step2の成果物が揃っているか）\n",
        "# =========================================================\n",
        "required_step2_vars = [\"train_feat\", \"test_feat\", \"FEATURE_COLS\", \"CATEGORICAL_COLS\"]\n",
        "for v in required_step2_vars:\n",
        "    if v not in globals():\n",
        "        raise NameError(f\"Missing required variable from Step2: {v}\")\n",
        "\n",
        "# targetの存在確認\n",
        "if TARGET_COL not in train_feat.columns:\n",
        "    raise KeyError(f\"train_feat missing target column: {TARGET_COL}\")"
      ],
      "metadata": {
        "id": "QR2D8BTps1eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 3-1. 学習行列の作成（Step4でそのまま使う）\n",
        "# =========================================================\n",
        "X_train = train_feat[FEATURE_COLS].copy()\n",
        "y_train = train_feat[TARGET_COL].astype(float).copy()\n",
        "\n",
        "X_test  = test_feat[FEATURE_COLS].copy()\n",
        "\n",
        "# LightGBMに渡すカテゴリ列（存在するものだけ）\n",
        "CAT_COLS = [c for c in CATEGORICAL_COLS if c in X_train.columns]\n",
        "for c in CAT_COLS:\n",
        "    X_train[c] = X_train[c].astype(\"category\")\n",
        "    X_test[c]  = X_test[c].astype(\"category\")\n",
        "\n",
        "print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
        "print(\"num cat cols:\", len(CAT_COLS), CAT_COLS[:10])"
      ],
      "metadata": {
        "id": "XSH3UlBQs63P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 3-2. 時系列 valid の作成（README準拠：過去→未来）\n",
        "#   デフォルトは 2022 を valid にする\n",
        "# =========================================================\n",
        "VALID_YEAR = 2022\n",
        "\n",
        "if \"year\" not in train_feat.columns:\n",
        "    raise KeyError(\"train_feat must have 'year' (created in Step1)\")\n",
        "\n",
        "valid_mask = (train_feat[\"year\"] == VALID_YEAR)\n",
        "train_mask = (train_feat[\"year\"] < VALID_YEAR)\n",
        "\n",
        "if valid_mask.sum() == 0:\n",
        "    raise ValueError(f\"No rows found for VALID_YEAR={VALID_YEAR}. Check train_feat['year'].\")\n",
        "\n",
        "if train_mask.sum() == 0:\n",
        "    raise ValueError(f\"No training rows found for year < {VALID_YEAR}. Check train_feat['year'].\")\n",
        "\n",
        "print(\"train rows:\", int(train_mask.sum()), \" valid rows:\", int(valid_mask.sum()))"
      ],
      "metadata": {
        "id": "Vhm36O-TGdCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 3-3. 学習関数（log1pで学習、expで戻す）\n",
        "#   ※評価はStep4でMAPEなど多角的に行う\n",
        "# =========================================================\n",
        "def train_lgb_model(\n",
        "    X_tr: pd.DataFrame, y_tr: pd.Series,\n",
        "    X_va: pd.DataFrame, y_va: pd.Series,\n",
        "    cat_cols: list[str],\n",
        "    seed: int = SEED\n",
        ") -> lgb.Booster:\n",
        "\n",
        "    params = {\n",
        "        \"objective\": \"regression\",\n",
        "        \"metric\": \"mae\",  # 早期停止用（MAPEはStep4で計算）\n",
        "        \"learning_rate\": 0.05,\n",
        "        \"num_leaves\": 96,\n",
        "        \"min_data_in_leaf\": 80,\n",
        "        \"feature_fraction\": 0.85,\n",
        "        \"bagging_fraction\": 0.85,\n",
        "        \"bagging_freq\": 1,\n",
        "        \"lambda_l1\": 0.0,\n",
        "        \"lambda_l2\": 0.0,\n",
        "        \"seed\": seed,\n",
        "        \"verbosity\": -1,\n",
        "    }\n",
        "\n",
        "    # log1pターゲット\n",
        "    y_tr_log = np.log1p(y_tr.values)\n",
        "    y_va_log = np.log1p(y_va.values)\n",
        "\n",
        "    dtrain = lgb.Dataset(X_tr, label=y_tr_log, categorical_feature=cat_cols, free_raw_data=False)\n",
        "    dvalid = lgb.Dataset(X_va, label=y_va_log, categorical_feature=cat_cols, free_raw_data=False)\n",
        "\n",
        "    model = lgb.train(\n",
        "        params=params,\n",
        "        train_set=dtrain,\n",
        "        valid_sets=[dtrain, dvalid],\n",
        "        valid_names=[\"train\", \"valid\"],\n",
        "        num_boost_round=8000,\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=300, verbose=False),\n",
        "            lgb.log_evaluation(period=300)\n",
        "        ]\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "7Comaq9hqQ0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 3-4. 単一モデル or タイプ別モデル（切替可能）\n",
        "#   - まずは安定な「単一モデル」をデフォルトにする\n",
        "#   - あとで必要ならタイプ別に切り替え可能\n",
        "# =========================================================\n",
        "USE_TYPE_SPLIT = False  # まずは False 推奨（完成度・安定性優先）\n",
        "\n",
        "MODELS = {}  # Step4で使う\n",
        "\n",
        "# Step4で使う予測保持（raw）\n",
        "oof_pred_raw = np.full(len(train_feat), np.nan, dtype=float)\n",
        "test_pred_raw = np.zeros(len(test_feat), dtype=float)\n",
        "\n",
        "def predict_raw(model: lgb.Booster, X: pd.DataFrame) -> np.ndarray:\n",
        "    # log1p学習 → expm1で戻す\n",
        "    pred_log = model.predict(X, num_iteration=model.best_iteration)\n",
        "    pred = np.expm1(pred_log)\n",
        "    return pred\n",
        "\n",
        "if not USE_TYPE_SPLIT:\n",
        "    # ---- 単一モデル ----\n",
        "    model_all = train_lgb_model(\n",
        "        X_train.loc[train_mask], y_train.loc[train_mask],\n",
        "        X_train.loc[valid_mask], y_train.loc[valid_mask],\n",
        "        cat_cols=CAT_COLS,\n",
        "        seed=SEED\n",
        "    )\n",
        "    MODELS[\"all\"] = model_all\n",
        "\n",
        "    # valid予測（OOFの一部）\n",
        "    oof_pred_raw[valid_mask.values] = predict_raw(model_all, X_train.loc[valid_mask])\n",
        "\n",
        "    # test予測\n",
        "    test_pred_raw[:] = predict_raw(model_all, X_test)\n",
        "\n",
        "else:\n",
        "    # ---- building_type 別モデル ----\n",
        "    if BUILDING_TYPE_COL not in train_feat.columns:\n",
        "        raise KeyError(f\"USE_TYPE_SPLIT=True requires {BUILDING_TYPE_COL}\")\n",
        "\n",
        "    for t in sorted(train_feat[BUILDING_TYPE_COL].astype(str).unique()):\n",
        "        tr_t = train_mask & (train_feat[BUILDING_TYPE_COL].astype(str) == t)\n",
        "        va_t = valid_mask & (train_feat[BUILDING_TYPE_COL].astype(str) == t)\n",
        "\n",
        "        if tr_t.sum() == 0 or va_t.sum() == 0:\n",
        "            print(f\"[skip] type={t} has tr={int(tr_t.sum())} va={int(va_t.sum())}\")\n",
        "            continue\n",
        "\n",
        "        model_t = train_lgb_model(\n",
        "            X_train.loc[tr_t], y_train.loc[tr_t],\n",
        "            X_train.loc[va_t], y_train.loc[va_t],\n",
        "            cat_cols=CAT_COLS,\n",
        "            seed=SEED + (hash(t) % 1000)\n",
        "        )\n",
        "        MODELS[t] = model_t\n",
        "\n",
        "        oof_pred_raw[va_t.values] = predict_raw(model_t, X_train.loc[va_t])\n",
        "\n",
        "        # test側も同じtypeのみ予測して埋める\n",
        "        te_t = (test_feat[BUILDING_TYPE_COL].astype(str) == t)\n",
        "        if te_t.sum() > 0:\n",
        "            test_pred_raw[te_t.values] = predict_raw(model_t, X_test.loc[te_t])"
      ],
      "metadata": {
        "id": "RLzwmUKeqUy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 3-5. Step4への受け渡し（列として保存）\n",
        "# =========================================================\n",
        "train_feat[\"pred_valid_raw\"] = oof_pred_raw   # valid年（例：2022）だけ値が入り、それ以外はNaN\n",
        "test_feat[\"pred_test_raw\"]   = test_pred_raw  # test全行\n",
        "\n",
        "PRED_VALID_COL = \"pred_valid_raw\"\n",
        "PRED_TEST_COL  = \"pred_test_raw\"\n",
        "\n",
        "print(\"[OK] Step3 artifacts ready:\")\n",
        "print(\" - MODELS keys:\", list(MODELS.keys())[:10])\n",
        "print(\" - train_feat[PRED_VALID_COL] non-null:\", int(np.isfinite(train_feat[PRED_VALID_COL]).sum()))\n",
        "print(\" - test_feat[PRED_TEST_COL] shape:\", test_feat[PRED_TEST_COL].shape)"
      ],
      "metadata": {
        "id": "rGud5_oyqXXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 4) 受け渡し検証・可視化・後処理・提出（完成度重視）\n",
        "#  - 1/2/3/4 の整合性を最初に厳密チェック\n",
        "#  - VALID_YEAR(=2022)で多角的検証\n",
        "#  - NaNフラグ（distance）に関する診断\n",
        "#  - 後処理（低価格補正など）を「改善したか」まで確認\n",
        "#  - submit.csv を作成\n",
        "# =========================================================\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "g_ByR_IU8bly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 4-0. 整合性チェック\n",
        "# =========================================================\n",
        "def assert_pipeline_integrity():\n",
        "    # Step1\n",
        "    for v in [\"train\", \"test\", \"ID_COL\", \"TARGET_COL\", \"YM_COL\", \"BUILDING_TYPE_COL\",\n",
        "              \"LON_COL\", \"LAT_COL\", \"DISTANCE_COLS\", \"DIST_FILL_QUANTILE\"]:\n",
        "        if v not in globals():\n",
        "            raise NameError(f\"Missing from Step1: {v}\")\n",
        "\n",
        "    # Step2\n",
        "    for v in [\"train_feat\", \"test_feat\", \"FEATURE_COLS\", \"CATEGORICAL_COLS\", \"DIST_FILL_VALUES\"]:\n",
        "        if v not in globals():\n",
        "            raise NameError(f\"Missing from Step2: {v}\")\n",
        "\n",
        "    # Step3\n",
        "    for v in [\"X_train\", \"y_train\", \"X_test\", \"MODELS\", \"valid_mask\", \"train_mask\",\n",
        "              \"PRED_VALID_COL\", \"PRED_TEST_COL\"]:\n",
        "        if v not in globals():\n",
        "            raise NameError(f\"Missing from Step3: {v}\")\n",
        "\n",
        "    # 列存在チェック\n",
        "    must_train_cols = [ID_COL, TARGET_COL, YM_COL, \"year\", \"month\", \"elapsed_months\", BUILDING_TYPE_COL, PRED_VALID_COL]\n",
        "    must_test_cols  = [ID_COL, YM_COL, \"year\", \"month\", \"elapsed_months\", BUILDING_TYPE_COL, PRED_TEST_COL]\n",
        "    for c in must_train_cols:\n",
        "        if c not in train_feat.columns:\n",
        "            raise KeyError(f\"train_feat missing: {c}\")\n",
        "    for c in must_test_cols:\n",
        "        if c not in test_feat.columns:\n",
        "            raise KeyError(f\"test_feat missing: {c}\")\n",
        "\n",
        "    # FEATURE_COLS は train/test 両方に存在していること\n",
        "    missing_train = [c for c in FEATURE_COLS if c not in train_feat.columns]\n",
        "    missing_test  = [c for c in FEATURE_COLS if c not in test_feat.columns]\n",
        "    if missing_train:\n",
        "        raise KeyError(f\"train_feat missing FEATURE_COLS: {missing_train[:20]} ... total={len(missing_train)}\")\n",
        "    if missing_test:\n",
        "        raise KeyError(f\"test_feat missing FEATURE_COLS: {missing_test[:20]} ... total={len(missing_test)}\")\n",
        "\n",
        "    # 予測列の NaN 状態：valid年以外はNaNでもOK、valid年は全て埋まっていること\n",
        "    if train_feat.loc[valid_mask, PRED_VALID_COL].isna().any():\n",
        "        n = int(train_feat.loc[valid_mask, PRED_VALID_COL].isna().sum())\n",
        "        raise ValueError(f\"Validation predictions contain NaN: {n} rows in VALID_YEAR={VALID_YEAR}\")\n",
        "\n",
        "    # test予測は全行埋まっていること\n",
        "    if test_feat[PRED_TEST_COL].isna().any():\n",
        "        n = int(test_feat[PRED_TEST_COL].isna().sum())\n",
        "        raise ValueError(f\"Test predictions contain NaN: {n} rows\")\n",
        "\n",
        "    # 数値のinfチェック（重要）\n",
        "    def _has_inf(df, cols):\n",
        "        arr = df[cols].select_dtypes(include=[np.number]).to_numpy()\n",
        "        return np.isinf(arr).any()\n",
        "\n",
        "    if _has_inf(train_feat, FEATURE_COLS + [PRED_VALID_COL, TARGET_COL]):\n",
        "        raise ValueError(\"train_feat contains inf in features/preds/target\")\n",
        "    if _has_inf(test_feat, FEATURE_COLS + [PRED_TEST_COL]):\n",
        "        raise ValueError(\"test_feat contains inf in features/preds\")\n",
        "\n",
        "    print(\"[OK] Pipeline integrity check passed.\")\n",
        "\n",
        "assert_pipeline_integrity()"
      ],
      "metadata": {
        "id": "uDUK-gZf8e9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 4-1. 指標関数\n",
        "# =========================================================\n",
        "EPS = 1e-6\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    denom = np.maximum(np.abs(y_true), EPS)\n",
        "    return np.mean(np.abs(y_true - y_pred) / denom)\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    return np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    return np.sqrt(np.mean((y_true - y_pred) ** 2))"
      ],
      "metadata": {
        "id": "81tyKwPSqZjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 4-2. VALID データセットの切り出し\n",
        "# =========================================================\n",
        "valid_df = train_feat.loc[valid_mask, :].copy()\n",
        "\n",
        "y_true = valid_df[TARGET_COL].values\n",
        "y_pred_raw = valid_df[PRED_VALID_COL].values\n",
        "\n",
        "print(\"\\n==== VALID METRICS (RAW) ====\")\n",
        "print(\"VALID_YEAR:\", VALID_YEAR)\n",
        "print(\"MAPE:\", mape(y_true, y_pred_raw))\n",
        "print(\"MAE :\", mae(y_true, y_pred_raw))\n",
        "print(\"RMSE:\", rmse(y_true, y_pred_raw))"
      ],
      "metadata": {
        "id": "aaM1NflE8neh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 4-3. 基本可視化（予測vs真値、残差、APE）\n",
        "# =========================================================\n",
        "def plot_pred_vs_true(y_true, y_pred, title):\n",
        "    plt.figure()\n",
        "    plt.scatter(y_true, y_pred, s=6, alpha=0.4)\n",
        "    mn = float(min(y_true.min(), y_pred.min()))\n",
        "    mx = float(max(y_true.max(), y_pred.max()))\n",
        "    plt.plot([mn, mx], [mn, mx])\n",
        "    plt.xlabel(\"true\")\n",
        "    plt.ylabel(\"pred\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def plot_hist(data, bins, title, xlabel):\n",
        "    plt.figure()\n",
        "    plt.hist(data, bins=bins)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(\"count\")\n",
        "    plt.show()\n",
        "\n",
        "plot_pred_vs_true(y_true, y_pred_raw, f\"VALID {VALID_YEAR}: Pred vs True (RAW)\")\n",
        "\n",
        "resid = y_pred_raw - y_true\n",
        "ape   = np.abs(resid) / np.maximum(np.abs(y_true), EPS)\n",
        "\n",
        "plot_hist(resid, bins=60, title=f\"VALID {VALID_YEAR}: Residual (pred-true)\", xlabel=\"residual\")\n",
        "plot_hist(ape,   bins=60, title=f\"VALID {VALID_YEAR}: APE\", xlabel=\"APE\")\n",
        "\n",
        "# ログ空間でも見る（外れの見え方が変わる）\n",
        "plot_pred_vs_true(np.log1p(y_true), np.log1p(y_pred_raw), f\"VALID {VALID_YEAR}: log1p(Pred) vs log1p(True)\")"
      ],
      "metadata": {
        "id": "TlSHkQ1hqcIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 4-4. 分解検証（改善の見通し用）\n",
        "#   - 月別 / 建物タイプ別 / 価格帯別（MAPEに効く）\n",
        "# =========================================================\n",
        "valid_df[\"ape\"] = ape\n",
        "\n",
        "# 月別\n",
        "month_summary = valid_df.groupby(\"month\").agg(\n",
        "    n=(TARGET_COL, \"size\"),\n",
        "    mape=(\"ape\", \"mean\"),\n",
        "    true_mean=(TARGET_COL, \"mean\"),\n",
        "    pred_mean=(PRED_VALID_COL, \"mean\"),\n",
        ")\n",
        "print(\"\\n---- VALID: month summary ----\")\n",
        "display(month_summary)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(month_summary.index.values, month_summary[\"mape\"].values, marker=\"o\")\n",
        "plt.xlabel(\"month\")\n",
        "plt.ylabel(\"MAPE\")\n",
        "plt.title(f\"VALID {VALID_YEAR}: MAPE by month\")\n",
        "plt.show()\n",
        "\n",
        "# building_type別\n",
        "type_summary = valid_df.groupby(BUILDING_TYPE_COL).agg(\n",
        "    n=(TARGET_COL, \"size\"),\n",
        "    mape=(\"ape\", \"mean\"),\n",
        "    true_mean=(TARGET_COL, \"mean\"),\n",
        "    pred_mean=(PRED_VALID_COL, \"mean\"),\n",
        ")\n",
        "print(\"\\n---- VALID: building_type summary ----\")\n",
        "display(type_summary)\n",
        "\n",
        "# 価格帯別（decile）\n",
        "valid_df[\"price_decile\"] = pd.qcut(valid_df[TARGET_COL], q=10, duplicates=\"drop\")\n",
        "price_summary = valid_df.groupby(\"price_decile\").agg(\n",
        "    n=(TARGET_COL, \"size\"),\n",
        "    mape=(\"ape\", \"mean\"),\n",
        "    true_mean=(TARGET_COL, \"mean\"),\n",
        "    pred_mean=(PRED_VALID_COL, \"mean\"),\n",
        ")\n",
        "print(\"\\n---- VALID: price decile summary ----\")\n",
        "display(price_summary)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(np.arange(len(price_summary)), price_summary[\"mape\"].values, marker=\"o\")\n",
        "plt.xlabel(\"price decile (low -> high)\")\n",
        "plt.ylabel(\"MAPE\")\n",
        "plt.title(f\"VALID {VALID_YEAR}: MAPE by price decile\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h_hPUSZp3t27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 4-5. distance NaNフラグ診断（重大：今回の改善点の核心）\n",
        "#   - NaN群と非NaN群でMAPEがどう違うか\n",
        "#   - どの距離列のNaNが特に効いているかが見える\n",
        "# =========================================================\n",
        "nan_diag_rows = []\n",
        "for c in DISTANCE_COLS:\n",
        "    flag = f\"{c}_is_nan\"\n",
        "    if flag not in valid_df.columns:\n",
        "        continue\n",
        "\n",
        "    m_nan = valid_df[flag] == 1\n",
        "    m_non = valid_df[flag] == 0\n",
        "\n",
        "    # 両方の群が存在しない場合はスキップ\n",
        "    if m_nan.sum() == 0 or m_non.sum() == 0:\n",
        "        continue\n",
        "\n",
        "    y_nan_true = valid_df.loc[m_nan, TARGET_COL].values\n",
        "    y_nan_pred = valid_df.loc[m_nan, PRED_VALID_COL].values\n",
        "\n",
        "    y_non_true = valid_df.loc[m_non, TARGET_COL].values\n",
        "    y_non_pred = valid_df.loc[m_non, PRED_VALID_COL].values\n",
        "\n",
        "    nan_diag_rows.append({\n",
        "        \"distance_col\": c,\n",
        "        \"nan_ratio\": float(m_nan.mean()),\n",
        "        \"mape_nan\": float(mape(y_nan_true, y_nan_pred)),\n",
        "        \"mape_non\": float(mape(y_non_true, y_non_pred)),\n",
        "        \"delta_mape(nan-non)\": float(mape(y_nan_true, y_nan_pred) - mape(y_non_true, y_non_pred)),\n",
        "        \"n_nan\": int(m_nan.sum()),\n",
        "        \"n_non\": int(m_non.sum()),\n",
        "    })\n",
        "\n",
        "nan_diag = pd.DataFrame(nan_diag_rows).sort_values(\"delta_mape(nan-non)\", ascending=False)\n",
        "print(\"\\n---- VALID: distance NaN diagnostic (top) ----\")\n",
        "display(nan_diag.head(30))\n",
        "\n",
        "# 代表的なものをプロット（上位5つ）\n",
        "topk = nan_diag.head(5)[\"distance_col\"].tolist()\n",
        "for c in topk:\n",
        "    flag = f\"{c}_is_nan\"\n",
        "    plt.figure()\n",
        "    grp = valid_df.groupby(flag)[\"ape\"].mean()\n",
        "    plt.bar(grp.index.astype(str), grp.values)\n",
        "    plt.title(f\"VALID {VALID_YEAR}: mean APE by {flag}\")\n",
        "    plt.xlabel(f\"{flag} (0=exists, 1=missing)\")\n",
        "    plt.ylabel(\"mean APE\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "S9DZIyifHARW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 4-6. 外れケースの抽出（次の改善が見える）\n",
        "#   - APE上位を building_id で追える\n",
        "# =========================================================\n",
        "worst = valid_df[[ID_COL, YM_COL, BUILDING_TYPE_COL, TARGET_COL, PRED_VALID_COL, \"ape\"]].sort_values(\"ape\", ascending=False)\n",
        "print(\"\\n---- Worst 30 APE (VALID) ----\")\n",
        "display(worst.head(30))"
      ],
      "metadata": {
        "id": "QLivWOc7HFMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 4-7. Feature importance（次の改善の方向性）\n",
        "#   - 単一モデルなら all\n",
        "#   - type split なら各モデル\n",
        "# =========================================================\n",
        "def show_feature_importance(model, feature_cols, topn=40, title=\"feature importance\"):\n",
        "    imp = pd.DataFrame({\n",
        "        \"feature\": feature_cols,\n",
        "        \"gain\": model.feature_importance(importance_type=\"gain\")\n",
        "    }).sort_values(\"gain\", ascending=False).head(topn)\n",
        "\n",
        "    plt.figure(figsize=(8, max(6, topn * 0.22)))\n",
        "    plt.barh(imp[\"feature\"][::-1], imp[\"gain\"][::-1])\n",
        "    plt.xlabel(\"gain importance\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    return imp\n",
        "\n",
        "if \"all\" in MODELS:\n",
        "    imp_all = show_feature_importance(MODELS[\"all\"], FEATURE_COLS, topn=40, title=\"Model(all): top gain importance\")\n",
        "    print(\"\\n---- Top features (all) ----\")\n",
        "    display(imp_all.head(25))\n",
        "else:\n",
        "    for k, mdl in MODELS.items():\n",
        "        imp_k = show_feature_importance(mdl, FEATURE_COLS, topn=30, title=f\"Model({k}): top gain importance\")\n",
        "        print(f\"\\n---- Top features ({k}) ----\")\n",
        "        display(imp_k.head(20))"
      ],
      "metadata": {
        "id": "NADEr7FhHHoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 4-8. 後処理（例：低価格帯補正）＋「改善したか」を検証\n",
        "#   ※ここはあなたの状況に応じて最適化ポイント\n",
        "# =========================================================\n",
        "LOW_TH = 9_000_000\n",
        "LOW_SCALE = 0.83\n",
        "\n",
        "def apply_low_price_scale(pred, th=LOW_TH, scale=LOW_SCALE):\n",
        "    pred2 = pred.copy().astype(float)\n",
        "    mask = pred2 <= th\n",
        "    pred2[mask] *= scale\n",
        "    return pred2\n",
        "\n",
        "y_pred_post = apply_low_price_scale(y_pred_raw)\n",
        "\n",
        "print(\"\\n==== VALID METRICS (POST: low-price scale) ====\")\n",
        "print(\"MAPE:\", mape(y_true, y_pred_post))\n",
        "print(\"MAE :\", mae(y_true, y_pred_post))\n",
        "print(\"RMSE:\", rmse(y_true, y_pred_post))\n",
        "\n",
        "plot_pred_vs_true(y_true, y_pred_post, f\"VALID {VALID_YEAR}: Pred vs True (POST)\")\n",
        "plot_hist(np.abs(y_pred_post - y_true)/np.maximum(np.abs(y_true), EPS), bins=60,\n",
        "          title=f\"VALID {VALID_YEAR}: APE (POST)\", xlabel=\"APE\")\n",
        "\n",
        "# 価格帯別に「後処理が効いたか」も見える化\n",
        "valid_df[\"ape_post\"] = np.abs(y_pred_post - y_true) / np.maximum(np.abs(y_true), EPS)\n",
        "price_post_summary = valid_df.groupby(\"price_decile\").agg(\n",
        "    mape_raw=(\"ape\", \"mean\"),\n",
        "    mape_post=(\"ape_post\", \"mean\"),\n",
        "    n=(\"ape\", \"size\")\n",
        ")\n",
        "print(\"\\n---- VALID: price decile (raw vs post) ----\")\n",
        "display(price_post_summary)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(np.arange(len(price_post_summary)), price_post_summary[\"mape_raw\"].values, marker=\"o\", label=\"raw\")\n",
        "plt.plot(np.arange(len(price_post_summary)), price_post_summary[\"mape_post\"].values, marker=\"o\", label=\"post\")\n",
        "plt.xlabel(\"price decile (low -> high)\")\n",
        "plt.ylabel(\"MAPE\")\n",
        "plt.title(f\"VALID {VALID_YEAR}: MAPE by price decile (raw vs post)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2bmTqiuzHMCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 4-9. submit作成（最終）\n",
        "#   - building_id と money_room を出す\n",
        "#   - raw/post どちらを採用するかはここで決められる\n",
        "# =========================================================\n",
        "# testの後処理も同じものを適用\n",
        "test_pred_raw = test_feat[PRED_TEST_COL].values\n",
        "test_pred_post = apply_low_price_scale(test_pred_raw)\n",
        "\n",
        "# 最終採用（まずは post を使う。必要なら raw に戻す）\n",
        "test_feat[\"pred_final\"] = test_pred_post\n",
        "\n",
        "# clip（価格として負値はありえない）\n",
        "test_feat[\"pred_final\"] = test_feat[\"pred_final\"].clip(lower=0)\n",
        "\n",
        "SUBMIT_PATH = DATA_DIR / \"submit.csv\"\n",
        "submit = test_feat[[ID_COL]].copy()\n",
        "submit[TARGET_COL] = test_feat[\"pred_final\"].values\n",
        "\n",
        "submit.to_csv(SUBMIT_PATH, index=False, encoding=\"utf-8\")\n",
        "print(f\"\\n[OK] saved submit: {SUBMIT_PATH}\")\n",
        "display(submit.head())"
      ],
      "metadata": {
        "id": "Nm2unO08HTc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 4-10. 次の改善が見通せる「自動診断まとめ」\n",
        "# =========================================================\n",
        "print(\"\\n==== Next Improvement Hints (auto) ====\")\n",
        "\n",
        "# 1) 月別の弱点\n",
        "worst_month = month_summary[\"mape\"].idxmax()\n",
        "print(f\"- Worst month (valid): month={int(worst_month)}  mape={month_summary.loc[worst_month, 'mape']:.4f}\")\n",
        "\n",
        "# 2) 価格帯の弱点\n",
        "worst_decile = price_summary[\"mape\"].idxmax()\n",
        "print(f\"- Worst price decile (valid): {worst_decile}  mape={price_summary.loc[worst_decile, 'mape']:.4f}\")\n",
        "\n",
        "# 3) distance NaN が痛い列\n",
        "if len(nan_diag) > 0:\n",
        "    top_bad = nan_diag.head(5)[[\"distance_col\", \"nan_ratio\", \"delta_mape(nan-non)\"]]\n",
        "    print(\"- distance NaN impact (top 5):\")\n",
        "    display(top_bad)\n",
        "    print(\"  Action: deltaが大きい列は、log距離に加えて、距離のbin化（例：0-200m,200-500m,...）や相互作用を検討\")\n",
        "\n",
        "# 4) elapsed_months の重要度が低いなら\n",
        "if \"all\" in MODELS:\n",
        "    # 重要度表に elapsed_months があるか確認\n",
        "    if \"elapsed_months\" in imp_all[\"feature\"].values:\n",
        "        rank = int(np.where(imp_all[\"feature\"].values == \"elapsed_months\")[0][0] + 1)\n",
        "        print(f\"- elapsed_months is in top list (rank~{rank} within shown topn).\")\n",
        "    else:\n",
        "        print(\"- elapsed_months not in shown top importance.\")\n",
        "        print(\"  Action: 時間水準特徴（例：prefecture×year の平均価格など）を追加すると効く余地が大きい\")\n",
        "\n",
        "print(\"\\n[Recommended next experiments]\")\n",
        "print(\"1) USE_TYPE_SPLIT=True（building_type別モデル）を試す（CVで確認）\")\n",
        "print(\"2) distance列のbin化＋is_nanの相互作用（例：is_nan * elapsed_months）\")\n",
        "print(\"3) 時間水準特徴：地域（pref/市区町村）× year の統計量（平均・中央値）を安全に作る（リークなしの形で）\")"
      ],
      "metadata": {
        "id": "qAzGhUzyHV01"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}